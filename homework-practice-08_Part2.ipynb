{"nbformat":4,"nbformat_minor":4,"metadata":{"language_info":{"version":"3.7.7","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","file_extension":".py"},"notebookId":"1915764a-dac4-48ee-a7ae-dfab694347c1","kernelspec":{"name":"python3","description":"IPython kernel implementation for Yandex DataSphere","spec":{"language":"python","display_name":"Yandex DataSphere Kernel","codemirror_mode":"python","argv":["/bin/true"],"env":{},"help_links":[]},"resources":{},"display_name":"Yandex DataSphere Kernel"},"ydsNotebookPath":"homework-practice-08_Part2.ipynb"},"cells":[{"cell_type":"markdown","source":"# Машинное обучение, ФКН ВШЭ\n\n## Практическое задание 8. Метод опорных векторов и аппроксимация ядер\n\n# ЧАСТЬ 2\n\n### Пришлось разбить на несколько файлов, чтобы иметь возможность обучать параллельно несколько заданий.","metadata":{"cellId":"d8252828-84f3-40cb-af66-89dcb05b847d"}},{"cell_type":"markdown","source":"Тестировать алгоритм мы будем на данных Fashion MNIST. Ниже код для их загрузки и подготовки.","metadata":{"cellId":"5c744995-060c-4ab5-be8b-d85904c5a913"}},{"cell_type":"code","source":"#!g1.1\nimport random\n\nimport numpy as np\nimport scipy\nfrom itertools import combinations\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import Normalizer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nimport lightgbm as lgbm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score","metadata":{"cellId":"b6f38de4-9964-471f-96fb-88222333805a","trusted":true},"outputs":[],"execution_count":385},{"cell_type":"code","source":"#!g1.1\nimport keras\nfrom keras.datasets import fashion_mnist\n(x_train_pics, y_train), (x_test_pics, y_test) = fashion_mnist.load_data()\nx_train = x_train_pics.reshape(x_train_pics.shape[0], -1)\nx_test = x_test_pics.reshape(x_test_pics.shape[0], -1)","metadata":{"cellId":"b8d0caeb-3c81-452e-a542-9d380dde6a6d","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"Using TensorFlow backend.\n"}],"execution_count":386},{"cell_type":"markdown","source":"__Задание 1. (5 баллов)__\n\nРеализуйте алгоритм, описанный выше. Можете воспользоваться шаблоном класса ниже или написать свой интерфейс.\n\nВаша реализация должна поддерживать следующие опции:\n1. Возможность задавать значения гиперпараметров new_dim (по умолчанию 50) и n_features (по умолчанию 1000).\n2. Возможность включать или выключать предварительное понижение размерности с помощью метода главных компонент.\n3. Возможность выбирать тип линейной модели (логистическая регрессия или SVM с линейным ядром).\n\nПротестируйте на данных Fashion MNIST, сформированных кодом выше. Если на тесте у вас получилась доля верных ответов не ниже 0.84 с гиперпараметрами по умолчанию, то вы всё сделали правильно.","metadata":{"cellId":"bacd0edb-085e-4b4e-b5d7-1a8e424695d5"}},{"cell_type":"code","source":"#!c1.8\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass RFFPipeline(BaseEstimator, TransformerMixin): \n    \"\"\"\n    При унаследовании от TransformerMixin, fit_transformer() можно не задавать, \n    Если добавить в качестве базового класса BaseEstimator и не реализовывать *args, **kwargs в конструкторе,\n    то будут доступны методы get_params() и set_params()\n    \"\"\"\n    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n        \"\"\"        \n        Implements pipeline, which consists of PCA decomposition,\n        Random Fourier Features approximation and linear classification model.\n        \n        n_features, int: amount of synthetic random features generated with RFF approximation.\n\n        new_dim, int: PCA output size.\n        \n        use_PCA, bool: whether to include PCA preprocessing.\n        \n        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n        \n        Feel free to edit this template for your preferences.    \n        \"\"\"\n        self.n_features = n_features\n        self.use_PCA = use_PCA\n        self.normalizer = Normalizer()\n        self.new_dim = new_dim\n        self.PCA_model = PCA(new_dim)\n        self.classifier = classifier\n        if self.classifier == 'logreg':\n            self.model = LogisticRegression(max_iter = 500, n_jobs = -1, random_state = 72)\n        else:\n            self.model = SVC(kernel='linear', random_state = 72)\n        \n    def fit(self, X, y):\n        \"\"\"\n        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n        \"\"\"\n        X = self.normalizer.fit_transform(X)\n        if self.use_PCA:\n            X = self.PCA_model.fit_transform(X, y)\n            print('PCA применён')\n            \n        elements_for_choosing = random.sample(range(X.shape[0]), 1420) # выбираю случ. элементы из выборки для составления пар\n        \n        dist_list = [] # список пар\n        for (c,p) in combinations(elements_for_choosing, 2): # составляю комбинации пар выборки\n            dist_list.append(list(scipy.spatial.distance.cdist(X[c].reshape(1,-1), \n                                                               X[p].reshape(1,-1),\n                                                               lambda u, v: ((u-v)**2).sum())[0]))\n        self.median = np.median(dist_list) # рассчитываю медиану\n        print('Медиана посчитана: {:.5f}'.format(self.median))\n        self.w = np.random.normal(scale=1./self.median, size=(X.shape[1], self.n_features)) # генерирую набор весов\n        self.b = np.random.uniform(-np.pi, np.pi, size=self.n_features) # генерирую набор свдигов\n        \n        new_features_X = np.cos(np.dot(X, self.w) + self.b)\n        self.model.fit(new_features_X, y)\n        return\n    \n    def predict_proba(self, X):\n        \"\"\"\n        Apply pipeline to obtain scores for input data.\n        \"\"\"\n        X = self.normalizer.transform(X)\n        if self.use_PCA:\n            X = self.PCA_model.transform(X)\n        \n        new_features_X = np.cos(np.dot(X, self.w) + self.b)\n        return self.model.predict_proba(new_features_X)\n        \n    def predict(self, X):\n        \"\"\"\n        Apply pipeline to obtain discrete predictions for input data.\n        \"\"\"\n        X = self.normalizer.transform(X)\n        if self.use_PCA:\n            X = self.PCA_model.transform(X)\n        \n        new_features_X = np.cos(np.dot(X, self.w) + self.b)\n        return self.model.predict(new_features_X)","metadata":{"cellId":"r8p2dof98ul4ph6cg81y","trusted":true},"outputs":[],"execution_count":402},{"cell_type":"markdown","source":"### Бонус","metadata":{"cellId":"f929d172-e9b7-4941-b8a4-9dd73bba2a2b"}},{"cell_type":"markdown","source":"__Задание 4. (Максимум 2 балла)__\n\nКак вы, должно быть, помните с курса МО-1, многие алгоритмы машинного обучения работают лучше, если признаки данных некоррелированы. Оказывается, что для RFF существует модификация, позволяющая получать ортогональные случайные признаки (Orthogonal Random Features, ORF). Об этом методе можно прочитать в [статье](https://proceedings.neurips.cc/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf). Реализуйте класс для вычисления ORF по аналогии с основным заданием. Обратите внимание, что ваш класс должен уметь работать со случаем n_features > new_dim (в статье есть замечание на этот счет). Проведите эксперименты, сравнивающие RFF и ORF, сделайте выводы.","metadata":{"cellId":"be06e29a-34de-4cae-a787-aa496be9b8ed"}},{"cell_type":"code","source":"#!c1.8\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass ORFPipeline(BaseEstimator, TransformerMixin): \n    \"\"\"\n    При унаследовании от TransformerMixin, fit_transformer() можно не задавать, \n    Если добавить в качестве базового класса BaseEstimator и не реализовывать *args, **kwargs в конструкторе,\n    то будут доступны методы get_params() и set_params()\n    \"\"\"\n    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n        \"\"\"        \n        Implements pipeline, which consists of PCA decomposition,\n        Random Fourier Features approximation and linear classification model.\n        \n        n_features, int: amount of synthetic random features generated with RFF approximation.\n\n        new_dim, int: PCA output size.\n        \n        use_PCA, bool: whether to include PCA preprocessing.\n        \n        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n        \n        Feel free to edit this template for your preferences.    \n        \"\"\"\n        self.n_features = n_features\n        self.use_PCA = use_PCA\n        self.normalizer = Normalizer()\n        self.new_dim = new_dim\n        self.PCA_model = PCA(new_dim)\n        self.classifier = classifier\n        if self.classifier == 'logreg':\n            self.model = LogisticRegression(max_iter = 500, n_jobs = -1, random_state = 72)\n        else:\n            self.model = SVC(kernel='linear', random_state = 72)\n        \n    def fit(self, X, y):\n        \"\"\"\n        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n        \"\"\"\n        X = self.normalizer.fit_transform(X)\n        if self.use_PCA:\n            X = self.PCA_model.fit_transform(X, y)\n            print('PCA применён')\n        \n        elements_for_choosing = random.sample(range(X.shape[0]), 1420) # выбираю случ. элементы из выборки для составления пар\n        dist_list = [] # список пар\n        \n        n_stacks = int(np.ceil(self.n_features/X.shape[1]))\n        \n        \n        if n_stacks == 1:\n            for (c,p) in combinations(elements_for_choosing, 2): # составляю комбинации пар выборки\n                dist_list.append(list(scipy.spatial.distance.cdist(X[c, :X.shape[1]].reshape(1,-1), \n                                                                   X[p, :X.shape[1]].reshape(1,-1),\n                                                                   lambda u, v: ((u-v)**2).sum())[0]))\n            self.median = np.median(dist_list) # рассчитываю медиану\n            print('Медиана посчитана: {:.5f}'.format(self.median))\n            w = np.random.normal(scale=1./self.median, size=(X.shape[1], self.n_features)) # генерирую набор весов\n            s = np.diag(chi.rvs(df = self.n_features, size = X.shape[1]))\n            self.w, _ = scipy.linalg.qr_multiply(w, s)\n            self.b = np.random.uniform(-np.pi, np.pi, size=self.n_features) # генерирую набор свдигов\n        \n        else:\n            for stck in range(n_stacks):\n                if ((stck+1) * X.shape[1]) > self.n_features:\n                    for (c,p) in combinations(elements_for_choosing, 2): # составляю комбинации пар выборки\n                        dist_list.append(list(scipy.spatial.distance.cdist(X[c, (stck+1) * X.shape[1]:].reshape(1,-1), \n                                                                           X[p, (stck+1) * X.shape[1]:].reshape(1,-1),\n                                                                           lambda u, v: ((u-v)**2).sum())[0]))\n                else:\n                    for (c,p) in combinations(elements_for_choosing, 2): # составляю комбинации пар выборки\n                        dist_list.append(list(scipy.spatial.distance.cdist(X[c, stck * X.shape[1]: (stck+1) * X.shape[1]].reshape(1,-1), \n                                                                           X[p, stck * X.shape[1]: (stck+1) * X.shape[1]].reshape(1,-1),\n                                                                           lambda u, v: ((u-v)**2).sum())[0]))\n                self.median = np.median(dist_list) # рассчитываю медиану\n                w = np.random.normal(scale=1./self.median, size=(X.shape[1], self.n_features)) # генерирую набор весов\n                s = np.diag(chi.rvs(df = self.n_features, size = X.shape[1]))\n                w_new, _ = scipy.linalg.qr_multiply(w, s)\n                if stck == 0:\n                    self.w = w_new\n                else:\n                    self.w = np.concatenate([self.w, w_new], axis=1)\n            self.b = np.random.uniform(-np.pi, np.pi, size=self.w.shape[1]) # генерирую набор свдигов\n\n        new_features_X = np.cos(np.dot(X, self.w) + self.b)\n        self.model.fit(new_features_X, y)\n        return\n    \n    def predict_proba(self, X):\n        \"\"\"\n        Apply pipeline to obtain scores for input data.\n        \"\"\"\n        X = self.normalizer.transform(X)\n        if self.use_PCA:\n            X = self.PCA_model.transform(X)\n        \n        new_features_X = np.cos(np.dot(X, self.w) + self.b)\n        return self.model.predict_proba(new_features_X)\n        \n    def predict(self, X):\n        \"\"\"\n        Apply pipeline to obtain discrete predictions for input data.\n        \"\"\"\n        X = self.normalizer.transform(X)\n        if self.use_PCA:\n            X = self.PCA_model.transform(X)\n        \n        new_features_X = np.cos(np.dot(X, self.w) + self.b)\n        return self.model.predict(new_features_X)","metadata":{"cellId":"7803e4f3-9145-4f6a-890e-0a542c9d7584","trusted":true},"outputs":[],"execution_count":444},{"cell_type":"markdown","source":"### Сравнение качества Orthogonal Random Features и Random Fourier Features на логистической регрессии","metadata":{"cellId":"c0gayl9mfrzgrzpu0lamm"}},{"cell_type":"code","source":"#!c1.8\n%%time\nrffp_kernel = ORFPipeline(n_features=300, new_dim=500)\nrffp_kernel.fit(x_train, y_train)\npreds = rffp_kernel.predict(x_test)\nprint('Orthogonal Random Features. Логистическая регрессия при new_dim > n_features. Доля верных ответов на тестовой выборке: {:.5f}\\n'.format(accuracy_score(preds, y_test)))","metadata":{"cellId":"v6bi2jlb3379jmfbimr5d","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"PCA применён\nМедиана посчитана: 0.77222\nOrthogonal Random Features. Логистическая регрессия при new_dim > n_features. Доля верных ответов на тестовой выборке: 0.85490\n\nCPU times: user 57.8 s, sys: 27.9 s, total: 1min 25s\nWall time: 2min 51s\n"}],"execution_count":445},{"cell_type":"code","source":"#!c1.8\n%%time\nrffp_kernel = RFFPipeline(n_features=300, new_dim=500)\nrffp_kernel.fit(x_train, y_train)\npreds = rffp_kernel.predict(x_test)\nprint('Random Fourier Features. Логистическая регрессия при new_dim > n_features. Доля верных ответов на тестовой выборке: {:.5f}\\n'.format(accuracy_score(preds, y_test)))","metadata":{"cellId":"q18yx99zoidux2jxnbjdv","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"PCA применён\nМедиана посчитана: 0.80286\nRandom Fourier Features. Логистическая регрессия при new_dim > n_features. Доля верных ответов на тестовой выборке: 0.84330\n\nCPU times: user 59.7 s, sys: 26.2 s, total: 1min 25s\nWall time: 2min 56s\n"}],"execution_count":420},{"cell_type":"code","source":"#!c1.8\n%%time\nrffp_kernel = ORFPipeline(n_features=500, new_dim=300)\nrffp_kernel.fit(x_train, y_train)\npreds = rffp_kernel.predict(x_test)\nprint('Orthogonal Random Features. Логистическая регрессия при new_dim < n_features. Доля верных ответов на тестовой выборке: {:.5f}\\n'.format(accuracy_score(preds, y_test)))","metadata":{"cellId":"exj268gnskg3o6ywy1ed3b","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"PCA применён\nOrthogonal Random Features. Логистическая регрессия при new_dim < n_features. Доля верных ответов на тестовой выборке: 0.86640\n\nCPU times: user 59.4 s, sys: 22.9 s, total: 1min 22s\nWall time: 5min 17s\n"}],"execution_count":448},{"cell_type":"code","source":"#!c1.8\n%%time\nrffp_kernel = RFFPipeline(n_features=500, new_dim=300)\nrffp_kernel.fit(x_train, y_train)\npreds = rffp_kernel.predict(x_test)\nprint('Random Fourier Features. Логистическая регрессия при new_dim < n_features. Доля верных ответов на тестовой выборке: {:.5f}\\n'.format(accuracy_score(preds, y_test)))","metadata":{"cellId":"h1x2joqowlh4u4br30u8","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"PCA применён\nМедиана посчитана: 0.78244\nRandom Fourier Features. Логистическая регрессия при new_dim < n_features. Доля верных ответов на тестовой выборке: 0.85970\n\nCPU times: user 41.6 s, sys: 21.3 s, total: 1min 2s\nWall time: 4min 17s\n"}],"execution_count":449},{"cell_type":"markdown","source":"### Сравнение качества Orthogonal Random Features и Random Fourier Features на линейном SVM","metadata":{"cellId":"vtkocnw3ra8sn0y9h5wgu"}},{"cell_type":"code","source":"#!c1.8\n%%time\nlinear_kernel = ORFPipeline(n_features=300, new_dim=500, classifier='svm')\nlinear_kernel.fit(x_train, y_train)\npreds = linear_kernel.predict(x_test)\nprint('Orthogonal Random Features. Линейный SVM при new_dim > n_features. Доля верных ответов на тестовой выборке: {:.5f}\\n'.format(accuracy_score(preds, y_test)))","metadata":{"cellId":"p319emg4l3e482edsrx2p","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"PCA применён\nМедиана посчитана: 0.79854\nOrthogonal Random Features. Линейный SVM при new_dim > n_features. Доля верных ответов на тестовой выборке: 0.85270\n\nCPU times: user 6min 58s, sys: 27.1 s, total: 7min 25s\nWall time: 6min 31s\n"}],"execution_count":450},{"cell_type":"code","source":"#!c1.8\n%%time\nlinear_kernel = RFFPipeline(n_features=300, new_dim=500, classifier='svm')\nlinear_kernel.fit(x_train, y_train)\npreds = linear_kernel.predict(x_test)\nprint('Random Fourier Features. Линейный SVM при new_dim > n_features. Доля верных ответов на тестовой выборке: {:.5f}\\n'.format(accuracy_score(preds, y_test)))","metadata":{"cellId":"k8aok13g3axfe7fphdq7d","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"PCA применён\nМедиана посчитана: 0.79058\nRandom Fourier Features. Линейный SVM при new_dim > n_features. Доля верных ответов на тестовой выборке: 0.85220\n\nCPU times: user 8min 42s, sys: 25.6 s, total: 9min 7s\nWall time: 8min 15s\n"}],"execution_count":451},{"cell_type":"code","source":"#!c1.8\n%%time\nlinear_kernel = ORFPipeline(n_features=500, new_dim=300, classifier='svm')\nlinear_kernel.fit(x_train, y_train)\npreds = linear_kernel.predict(x_test)\nprint('Orthogonal Random Features. Линейный SVM при new_dim < n_features. Доля верных ответов на тестовой выборке: {:.5f}\\n'.format(accuracy_score(preds, y_test)))","metadata":{"cellId":"nx69nzz8zo86a3ukgl62","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"PCA применён\nOrthogonal Random Features. Логистическая регрессия при new_dim < n_features. Доля верных ответов на тестовой выборке: 0.86630\n\nCPU times: user 13min 9s, sys: 20.9 s, total: 13min 30s\nWall time: 12min 51s\n"}],"execution_count":452},{"cell_type":"code","source":"#!c1.8\n%%time\nlinear_kernel = RFFPipeline(n_features=500, new_dim=300, classifier='svm')\nlinear_kernel.fit(x_train, y_train)\npreds = linear_kernel.predict(x_test)\nprint('Random Fourier Features. Линейный SVM при new_dim < n_features. Доля верных ответов на тестовой выборке: {:.5f}\\n'.format(accuracy_score(preds, y_test)))","metadata":{"cellId":"rp7zx66k7pmcybslrmv2j4","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"PCA применён\nМедиана посчитана: 0.75571\nRandom Fourier Features. Линейный SVM при new_dim < n_features. Доля верных ответов на тестовой выборке: 0.86330\n\nCPU times: user 11min 48s, sys: 20.2 s, total: 12min 9s\nWall time: 11min 32s\n"}],"execution_count":453},{"cell_type":"markdown","source":"**Вывод:** на обоих моделях: и линейном SVM и логистической регрессии при разных соотношениях new_dim <> n_features, как и ожидалось по результатам реализации алгоритма из статьи, Orthogonal Random Features показал лучшее качество, чем Random Fourier Features за счёт отсутствия у признаков высокой скоррелированности.","metadata":{"cellId":"z03vvt0dq66kjmg1aecjm"}}]}